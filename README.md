# ğŸ” Attention MekanizmasÄ±nÄ±n GÃ¶rselleÅŸtirmesi
## ğŸ“Œ Ä°Ã§erik

- Self-Attention mekanizmasÄ±
- Causal (Nedensel) Attention
- Multi-Head Attention (2 baÅŸlÄ±kla)
- GÃ¶rselleÅŸtirme: Heatmap ile aÄŸÄ±rlÄ±k matrisleri

## ğŸ§  KullanÄ±lan YÃ¶ntemler

### 1. Self-Attention
Token vektÃ¶rlerinin kendi aralarÄ±nda benzerliÄŸini Ã¶lÃ§er ve her bir tokenâ€™Ä±n diÄŸer tokenâ€™larla olan baÄŸÄ±nÄ± deÄŸerlendirir.

### 2. Causal Attention
Dil modeli gibi sÄ±raya duyarlÄ± yapÄ±larda kullanÄ±lÄ±r. Her token sadece kendisinden Ã¶nceki ve kendisiyle olan iliÅŸkilere bakar.

### 3. Multi-Head Attention
VektÃ¶rÃ¼ farklÄ± baÅŸlÄ±klara bÃ¶lerek her birinin farklÄ± perspektiflerden iliÅŸkileri Ã¶ÄŸrenmesini saÄŸlar.

## ğŸ“Š Ã–rnek GÃ¶rselleÅŸtirmeler

### ğŸ¯ Self-Attention AÄŸÄ±rlÄ±k Matrisi
Token'lar arasÄ± dikkat daÄŸÄ±lÄ±mÄ±:

![Self-Attention](./images/self_attention.png)

### â³ Causal Attention AÄŸÄ±rlÄ±k Matrisi
SÄ±ra bilgisine duyarlÄ± dikkat daÄŸÄ±lÄ±mÄ±:

![Causal-Attention](./images/causal_attention.png)

### ğŸ§© Multi-Head Attention
Her bir head iÃ§in farklÄ± dikkat Ã¶rÃ¼ntÃ¼leri:

**Head 1**  
![Head 1](./images/head1.png)

**Head 2**  
![Head 2](./images/head2.png)

## ğŸ› ï¸ Kurulum ve KullanÄ±m

1. Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install numpy matplotlib seaborn
```

2. Python dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python attention.py
```
## ğŸ¤– KatkÄ±
Bu proje, dikkat mekanizmalarÄ±nÄ± Ã¶ÄŸrenmek isteyen Ã¶ÄŸrenciler ve araÅŸtÄ±rmacÄ±lar iÃ§in bir Ã¶ÄŸretici araÃ§tÄ±r. KatkÄ±da bulunmak isterseniz PR gÃ¶nderebilirsiniz.
